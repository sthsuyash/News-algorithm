{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTOlyAIrQYOl"
   },
   "source": [
    "# Nepali Sentiment analysis with transformer architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet gensim nltk optuna seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l7FwPpcXQWS9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8933</th>\n",
       "      <td>कोभिड का कारण नौ महिनापछि नेपाली राष्ट्रिय क्र...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34053</th>\n",
       "      <td>इन्डियामा कोभिड केस बढ्नु हामिलाई पनि चिन्ताको...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22013</th>\n",
       "      <td>कोभिड नियन्त्रणमा सरकार गम्भीर नभएको आरोप</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27865</th>\n",
       "      <td>जापान कोभिड दैनिक अपडेट मे संक्रमितहरुको संख्य...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35258</th>\n",
       "      <td>कोभिड प्रभावित देशको भ्रमण नगर्न आफ्ना नागरिकल...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentences  Sentiment\n",
       "8933   कोभिड का कारण नौ महिनापछि नेपाली राष्ट्रिय क्र...          1\n",
       "34053  इन्डियामा कोभिड केस बढ्नु हामिलाई पनि चिन्ताको...          2\n",
       "22013          कोभिड नियन्त्रणमा सरकार गम्भीर नभएको आरोप          2\n",
       "27865  जापान कोभिड दैनिक अपडेट मे संक्रमितहरुको संख्य...          2\n",
       "35258  कोभिड प्रभावित देशको भ्रमण नगर्न आफ्ना नागरिकल...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/processed/combined_sentiment_dataset.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z0ipdsF5Qi-e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    15857\n",
       "2    14393\n",
       "0    10458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV99mYLhQv2E"
   },
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoYbXP16Qzb2",
    "outputId": "a6487c5e-5ace-4bc4-85fe-922b705a196c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Suyash\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('nepali'))\n",
    "not_stop_words = {'छैन', 'थिएन', 'यद्यपि', 'यसबाहेक'}\n",
    "stop_words -= not_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S_eJlZxxQ1Hb"
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str):\n",
    "    # Regular expression to keep only Nepali characters (Devanagari script)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    # Basic letter normalization\n",
    "    letters_to_normalize = {\n",
    "        \"ी\": \"ि\", \"ू\": \"ु\", \"श\": \"स\", \"ष\": \"स\", \"व\": \"ब\", \"ङ\": \"न\", \"ञ\": \"न\", \"ण\": \"न\", \"ृ\": \"र\",\n",
    "        \"ँ\": \"\", \"ं\": \"\", \"ः\": \"\", \"ं\": \"\"\n",
    "    }\n",
    "    for l1, l2 in letters_to_normalize.items():\n",
    "        text = text.replace(l1, l2)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zdrEm42zQ3IU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 40708/40708 [00:00<00:00, 66849.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['Sentences'] = df['Sentences'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "4sRVKRagQ7G2",
    "outputId": "174701a1-80a3-4c9c-ed1c-2848484d64b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10478</th>\n",
       "      <td>कोभिड हाम्रो कहानि</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24651</th>\n",
       "      <td>कोरोना भाइरस गुल्मिका पुरुसको कोभिड कारन बुटबल...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34373</th>\n",
       "      <td>बिज्नको सुझाबअनुसार कोभिड रोकथाम नियन्त्रन मन्...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14385</th>\n",
       "      <td>कोरोना भाइरस कोभिड सक्रमन रोकथाम नियन्त्रनका म...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11558</th>\n",
       "      <td>अभिभाबक त्रसित जनाउदै काकडभिट्टा माध्यमिक बिद्...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentences  Sentiment\n",
       "10478                                 कोभिड हाम्रो कहानि          0\n",
       "24651  कोरोना भाइरस गुल्मिका पुरुसको कोभिड कारन बुटबल...          2\n",
       "34373  बिज्नको सुझाबअनुसार कोभिड रोकथाम नियन्त्रन मन्...          1\n",
       "14385  कोरोना भाइरस कोभिड सक्रमन रोकथाम नियन्त्रनका म...          2\n",
       "11558  अभिभाबक त्रसित जनाउदै काकडभिट्टा माध्यमिक बिद्...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmGE1HCWQ9-Z"
   },
   "source": [
    "## Tokanization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8FINpZgVQ7tS"
   },
   "outputs": [],
   "source": [
    "sentences = [item for item in df['Sentences']]\n",
    "sentiments = [item for item in df['Sentiment']]\n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fnZrUUuaRHBz"
   },
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=64, window=5, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DHEyGeLBRHZ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Reviews Shape: (40708, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "# # Convert each token to its word2vec embedding\n",
    "# max_length = 32\n",
    "# embedded_reviews = []\n",
    "\n",
    "# for tokens in tokenized_sentences:\n",
    "#     review_vector = []\n",
    "#     for token in tokens:\n",
    "#         if token in word2vec_model.wv:\n",
    "#             review_vector.append(word2vec_model.wv[token])\n",
    "#         else:\n",
    "#             # If token not found, use a zero vector\n",
    "#             review_vector.append(np.zeros(word2vec_model.vector_size))\n",
    "#     # Pad the review to max_length with zero vectors\n",
    "#     if len(review_vector) > max_length:\n",
    "#         review_vector = review_vector[:max_length]\n",
    "#     else:\n",
    "#         review_vector.extend([np.zeros(word2vec_model.vector_size)] * (max_length - len(review_vector)))\n",
    "\n",
    "\n",
    "#     embedded_reviews.append(review_vector)\n",
    "\n",
    "max_length = 32\n",
    "embedded_reviews = []\n",
    "\n",
    "for tokens in tokenized_sentences:\n",
    "    review_vector = [word2vec_model.wv[token] if token in word2vec_model.wv else np.zeros(\n",
    "        word2vec_model.vector_size) for token in tokens]\n",
    "    review_vector = review_vector[:max_length] if len(review_vector) > max_length else review_vector + [\n",
    "        np.zeros(word2vec_model.vector_size)] * (max_length - len(review_vector))\n",
    "    embedded_reviews.append(review_vector)\n",
    "\n",
    "embedded_reviews_array = np.array(embedded_reviews)\n",
    "print(\"Embedded Reviews Shape:\", embedded_reviews_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0dpiuTISdaN"
   },
   "source": [
    "# Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IVN4DBmhbji_"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=32):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * math.sqrt(self.d_model) + self.pe[:x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, v)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        bs = q.size(0)\n",
    "        q = self.q_linear(q).view(bs, -1, self.num_heads,\n",
    "                                  self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(bs, -1, self.num_heads,\n",
    "                                  self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.num_heads,\n",
    "                                  self.d_k).transpose(1, 2)\n",
    "        scores = self.attention(q, k, v)\n",
    "        concat = scores.transpose(1, 2).contiguous().view(\n",
    "            bs, -1, self.num_heads * self.d_k)\n",
    "        return self.out(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                nn.ReLU(), nn.Linear(d_ff, d_model))\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout2(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uCL9HaATcDZZ"
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, d_ff, max_len, dropout):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_monIs90cLIL"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "d_ff = 128\n",
    "max_len = max_length\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hVF1kt3zcLhx"
   },
   "outputs": [],
   "source": [
    "# Model, Criterion, Optimizer\n",
    "model = SentimentAnalysisModel(\n",
    "    d_model, num_heads, num_layers, num_classes, d_ff, max_len, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "il8M2zw7czge"
   },
   "outputs": [],
   "source": [
    "# Convert data to tensors and create DataLoader\n",
    "sentiments_tensor = torch.tensor(sentiments, dtype=torch.long)\n",
    "embedded_reviews_tensor = torch.tensor(\n",
    "    embedded_reviews_array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Split\n",
    "dataset = TensorDataset(embedded_reviews_tensor, sentiments_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6P7cyVOrdDYW"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:282\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    274\u001b[0m     (inputs,)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    281\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 282\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:161\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    155\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    160\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 161\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 16\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training Phase\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    val_accuracy = correct_predictions / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation on Test Set\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(actuals, predictions)\n",
    "precision = precision_score(actuals, predictions)\n",
    "recall = recall_score(actuals, predictions)\n",
    "conf_matrix = confusion_matrix(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report and Confusion Matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(actuals, predictions, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src/models/transformer_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
